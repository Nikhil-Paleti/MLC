{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca3cfe7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf97d559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 1) BatchNorm (N, C, *spatial)\n",
    "# ----------------------------\n",
    "class BatchNormManual(nn.Module):\n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True, track_running_stats=True):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.affine = affine\n",
    "        self.track_running_stats = track_running_stats\n",
    "\n",
    "        if affine:\n",
    "            self.weight = nn.Parameter(torch.ones(num_features))  # gamma\n",
    "            self.bias   = nn.Parameter(torch.zeros(num_features)) # beta\n",
    "        else:\n",
    "            self.register_parameter('weight', None)\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "        if track_running_stats:\n",
    "            self.register_buffer('running_mean', torch.zeros(num_features))\n",
    "            self.register_buffer('running_var',  torch.ones(num_features))\n",
    "            self.register_buffer('num_batches_tracked', torch.tensor(0, dtype=torch.long))\n",
    "        else:\n",
    "            self.register_parameter('running_mean', None)\n",
    "            self.register_parameter('running_var',  None)\n",
    "            self.register_parameter('num_batches_tracked', None)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (N, C, *S)\n",
    "        original_dtype = x.dtype\n",
    "        x_float = x.float()\n",
    "\n",
    "        # axes to reduce: all except channel (dim=1)\n",
    "        reduce_dims = [0] + list(range(2, x.dim()))\n",
    "        if self.training:\n",
    "            # batch statistics\n",
    "            batch_mean = x_float.mean(dim=reduce_dims, keepdim=False)\n",
    "            batch_var  = x_float.var(dim=reduce_dims, unbiased=False, keepdim=False)\n",
    "\n",
    "            if self.track_running_stats:\n",
    "                with torch.no_grad():\n",
    "                    self.num_batches_tracked += 1\n",
    "                    # EMA update\n",
    "                    self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean\n",
    "                    self.running_var  = (1 - self.momentum) * self.running_var  + self.momentum * batch_var\n",
    "\n",
    "            mean = batch_mean\n",
    "            var  = batch_var\n",
    "        else:\n",
    "            # inference: use running stats if available, else fall back to batch stats\n",
    "            if self.track_running_stats and self.running_mean is not None:\n",
    "                mean = self.running_mean\n",
    "                var  = self.running_var\n",
    "            else:\n",
    "                mean = x_float.mean(dim=reduce_dims, keepdim=False)\n",
    "                var  = x_float.var(dim=reduce_dims, unbiased=False, keepdim=False)\n",
    "\n",
    "        # reshape (C,) -> (1,C,1,1,...) for broadcasting\n",
    "        shape = [1, -1] + [1] * (x.dim() - 2)\n",
    "        mean = mean.view(*shape)\n",
    "        var  = var.view(*shape)\n",
    "\n",
    "        x_hat = (x_float - mean) / torch.sqrt(var + self.eps)\n",
    "        if self.affine:\n",
    "            weight = self.weight.view(*shape).to(x_hat.dtype)\n",
    "            bias   = self.bias.view(*shape).to(x_hat.dtype)\n",
    "            y = weight * x_hat + bias\n",
    "        else:\n",
    "            y = x_hat\n",
    "\n",
    "        return y.to(original_dtype)\n",
    "\n",
    "\n",
    "# -----------------------------------\n",
    "# 2) LayerNorm over the last K dims\n",
    "# -----------------------------------\n",
    "class LayerNormManual(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5, elementwise_affine=True):\n",
    "        \"\"\"\n",
    "        normalized_shape: int or tuple of ints for the trailing dimensions to normalize.\n",
    "        Example: hidden_size (int) or (H, W) for 2D per-sample spatial LN.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if isinstance(normalized_shape, int):\n",
    "            normalized_shape = (normalized_shape,)\n",
    "        self.normalized_shape = tuple(normalized_shape)\n",
    "        self.eps = eps\n",
    "        self.elementwise_affine = elementwise_affine\n",
    "\n",
    "        if elementwise_affine:\n",
    "            self.weight = nn.Parameter(torch.ones(*self.normalized_shape))  # gamma\n",
    "            self.bias   = nn.Parameter(torch.zeros(*self.normalized_shape)) # beta\n",
    "        else:\n",
    "            self.register_parameter('weight', None)\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # reduce over the last len(normalized_shape) dims\n",
    "        reduce_dims = tuple(range(x.dim() - len(self.normalized_shape), x.dim()))\n",
    "        x_float = x.float()\n",
    "        mean = x_float.mean(dim=reduce_dims, keepdim=True)\n",
    "        var  = x_float.var(dim=reduce_dims, unbiased=False, keepdim=True)\n",
    "\n",
    "        x_hat = (x_float - mean) / torch.sqrt(var + self.eps)\n",
    "        if self.elementwise_affine:\n",
    "            y = x_hat * self.weight + self.bias\n",
    "        else:\n",
    "            y = x_hat\n",
    "        return y.to(x.dtype)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 3) RMSNorm (no centering)\n",
    "# ---------------------------\n",
    "class RMSNormManual(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-8, elementwise_affine=True):\n",
    "        \"\"\"\n",
    "        dim: size of the last (normalized) dimension.\n",
    "        Only gamma is typically used; beta is optional (default off here).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.eps = eps\n",
    "        self.elementwise_affine = elementwise_affine\n",
    "\n",
    "        if elementwise_affine:\n",
    "            self.weight = nn.Parameter(torch.ones(dim))  # gamma\n",
    "        else:\n",
    "            self.register_parameter('weight', None)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # normalize over last dimension\n",
    "        x_float = x.float()\n",
    "        # rms = sqrt(mean(x^2) + eps)\n",
    "        rms = torch.sqrt(x_float.pow(2).mean(dim=-1, keepdim=True) + self.eps)\n",
    "        y = x_float / rms\n",
    "        if self.elementwise_affine:\n",
    "            y = y * self.weight.view(*([1] * (x.dim() - 1)), self.dim)\n",
    "        return y.to(x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70cafe74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LN max diff: 4.76837158203125e-07\n",
      "BN train max diff: 4.76837158203125e-07\n",
      "BN eval max diff: 0.0001239776611328125\n",
      "RMSNorm output shape: torch.Size([2, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "# ----- LayerNorm check -----\n",
    "x = torch.randn(4, 16, 64)\n",
    "ln_ref = nn.LayerNorm(64)\n",
    "ln_my  = LayerNormManual(64)\n",
    "# copy params to compare apples-to-apples\n",
    "ln_my.weight.data.copy_(ln_ref.weight.data)\n",
    "ln_my.bias.data.copy_(ln_ref.bias.data)\n",
    "print(\"LN max diff:\", (ln_ref(x) - ln_my(x)).abs().max().item())\n",
    "\n",
    "# ----- BatchNorm2d check -----\n",
    "x = torch.randn(8, 32, 16, 16)\n",
    "bn_ref = nn.BatchNorm2d(32, affine=True, momentum=0.1, eps=1e-5, track_running_stats=True)\n",
    "bn_my  = BatchNormManual(32, affine=True, momentum=0.1, eps=1e-5, track_running_stats=True)\n",
    "# train mode: both update running stats from same batch\n",
    "bn_ref.train(); bn_my.train()\n",
    "# align parameters\n",
    "bn_my.weight.data.copy_(bn_ref.weight.data)\n",
    "bn_my.bias.data.copy_(bn_ref.bias.data)\n",
    "y_ref = bn_ref(x)\n",
    "y_my  = bn_my(x)\n",
    "print(\"BN train max diff:\", (y_ref - y_my).abs().max().item())\n",
    "\n",
    "# switch to eval: both should use their accumulated running stats\n",
    "bn_ref.eval(); bn_my.eval()\n",
    "y_ref2 = bn_ref(x)\n",
    "y_my2  = bn_my(x)\n",
    "print(\"BN eval max diff:\", (y_ref2 - y_my2).abs().max().item())\n",
    "\n",
    "# ----- RMSNorm check (no official ref, just shape/run) -----\n",
    "x = torch.randn(2, 5, 768, dtype=torch.bfloat16)  # bf16 friendly\n",
    "rms = RMSNormManual(768)\n",
    "y = rms(x)\n",
    "print(\"RMSNorm output shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72a6e72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
