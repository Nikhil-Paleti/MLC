{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6597033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e6d3ac",
   "metadata": {},
   "source": [
    "![image](https://www.ibm.com/content/dam/connectedassets-adobe-cms/worldwide-content/creative-assets/s-migr/ul/g/3f/b4/mha-mqa-and-gqa.component.xl-retina.ts=1744898975663.png/content/adobe-cms/us/en/think/topics/grouped-query-attention/jcr:content/root/table_of_contents/body-article-8/image_700444495)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f07900b",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 25 # batch size \n",
    "T = 50 # sequence length for Q \n",
    "S = T # sequence length for K,V \n",
    "D = 8*50 # d_model embedding dim \n",
    "N = 8 # no of query heads \n",
    "K = 4 # no of key, value heads \n",
    "H = D // N # attention head dim \n",
    "G = N // K # q heads per kv head \n",
    "\n",
    "x = torch.rand((B, T, D))\n",
    "\n",
    "Q_w = torch.rand((D, N, H))\n",
    "K_w = torch.rand((D, K, H))\n",
    "V_w = torch.rand((D, K, H))\n",
    "output_w = torch.rand((N, H, D))\n",
    "\n",
    "query = torch.einsum(\"BTD,DNH->BTNH\", x, Q_w)   # (B,T,N,H)\n",
    "key   = torch.einsum(\"BTD,DKH->BTKH\", x, K_w)   # (B,T,K,H)\n",
    "value = torch.einsum(\"BTD,DKH->BTKH\", x, V_w)   # (B,T,K,H)\n",
    "\n",
    "query = query.reshape(B, T, K, G, H)            # (B,T,K,G,H)\n",
    "\n",
    "attn_logits = torch.einsum(\"BTKGH,BSKH->BTSKG\", query, key)  # (B,T,S,K,G)\n",
    "scale = torch.sqrt(torch.tensor(H, dtype=attn_logits.dtype, device=attn_logits.device))\n",
    "attn_logits = attn_logits / scale\n",
    "\n",
    "causal = torch.ones(T, S, dtype=torch.bool, device=attn_logits.device).tril().view(1, T, S, 1, 1)\n",
    "attn_logits = attn_logits.masked_fill(~causal, float(\"-inf\"))\n",
    "\n",
    "attn_probs = torch.softmax(attn_logits, dim=2)               # (B,T,S,K,G)\n",
    "\n",
    "context = torch.einsum(\"BTSKG,BSKH->BTKGH\", attn_probs, value)  # (B,T,K,G,H)\n",
    "context = context.reshape(B, T, N, H)                           # (B,T,N,H)\n",
    "\n",
    "outputs = torch.einsum(\"BTNH,NHD->BTD\", context, output_w)      # (B,T,D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fa9ac0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
