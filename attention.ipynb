{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6597033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e6d3ac",
   "metadata": {},
   "source": [
    "![image](https://www.ibm.com/content/dam/connectedassets-adobe-cms/worldwide-content/creative-assets/s-migr/ul/g/3f/b4/mha-mqa-and-gqa.component.xl-retina.ts=1744898975663.png/content/adobe-cms/us/en/think/topics/grouped-query-attention/jcr:content/root/table_of_contents/body-article-8/image_700444495)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f07900b",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 25 # batch size \n",
    "T = 50 # sequence length for Q \n",
    "S = T # sequence length for K,V \n",
    "D = 8*50 # d_model embedding dim \n",
    "N = 8 # no of query heads \n",
    "K = 4 # no of key, value heads \n",
    "H = D // N # attention head dim \n",
    "G = N // K # q heads per kv head \n",
    "\n",
    "x = torch.rand((B, T, D))\n",
    "\n",
    "Q_w = torch.rand((D, N, H))\n",
    "K_w = torch.rand((D, K, H))\n",
    "V_w = torch.rand((D, K, H))\n",
    "output_w = torch.rand((N, H, D))\n",
    "\n",
    "query = torch.einsum(\"BTD,DNH->BTNH\", x, Q_w)   # (B,T,N,H)\n",
    "key   = torch.einsum(\"BTD,DKH->BTKH\", x, K_w)   # (B,T,K,H)\n",
    "value = torch.einsum(\"BTD,DKH->BTKH\", x, V_w)   # (B,T,K,H)\n",
    "\n",
    "query = query.reshape(B, T, K, G, H)            # (B,T,K,G,H)\n",
    "\n",
    "attn_logits = torch.einsum(\"BTKGH,BSKH->BTSKG\", query, key)  # (B,T,S,K,G)\n",
    "scale = torch.sqrt(torch.tensor(H, dtype=attn_logits.dtype, device=attn_logits.device))\n",
    "attn_logits = attn_logits / scale\n",
    "\n",
    "causal = torch.ones(T, S, dtype=torch.bool, device=attn_logits.device).tril().view(1, T, S, 1, 1)\n",
    "attn_logits = attn_logits.masked_fill(~causal, float(\"-inf\"))\n",
    "\n",
    "attn_probs = torch.softmax(attn_logits, dim=2)               # (B,T,S,K,G)\n",
    "\n",
    "context = torch.einsum(\"BTSKG,BSKH->BTKGH\", attn_probs, value)  # (B,T,K,G,H)\n",
    "context = context.reshape(B, T, N, H)                           # (B,T,N,H)\n",
    "\n",
    "outputs = torch.einsum(\"BTNH,NHD->BTD\", context, output_w)      # (B,T,D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fa9ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25, 50, 400])\n"
     ]
    }
   ],
   "source": [
    "B = 25 # batch size \n",
    "T = 50 # sequence length for Q \n",
    "S = T # sequence length for K,V \n",
    "D = 8*50 # d_model embedding dim \n",
    "N = 8 # no of query heads \n",
    "K = 4 # no of key, value heads \n",
    "H = D // N # attention head dim \n",
    "G = N // K # q heads per kv head \n",
    "\n",
    "x = torch.rand((B, T, D))\n",
    "\n",
    "Q_w = torch.rand((D, N, H))\n",
    "K_w = torch.rand((D, K, H))\n",
    "V_w = torch.rand((D, K, H))\n",
    "output_w = torch.rand((N, H, D))\n",
    "\n",
    "query = torch.einsum(\"BTD,DNH->BTNH\", x, Q_w)     # [B,T,N,H]\n",
    "key   = torch.einsum(\"BTD,DKH->BTKH\", x, K_w)     # [B,T,K,H]\n",
    "value = torch.einsum(\"BTD,DKH->BTKH\", x, V_w)     # [B,T,K,H]\n",
    "\n",
    "# GQA: expand K/V heads to N heads\n",
    "key   = key.repeat_interleave(G, dim=2)           # [B,T,N,H]\n",
    "value = value.repeat_interleave(G, dim=2)         # [B,T,N,H]\n",
    "\n",
    "attn_logits = torch.einsum(\"BTNH,BSNH->BTSN\", query, key)  # [B,T,S,N]\n",
    "scale = torch.sqrt(torch.tensor(H, dtype=attn_logits.dtype, device=attn_logits.device))\n",
    "attn_logits = attn_logits / scale\n",
    "\n",
    "# causal mask (lower triangle), broadcast to [B,T,S,N]\n",
    "mask = torch.ones(T, S, dtype=torch.bool).tril().view(1, T, S, 1)\n",
    "mask = mask.to(attn_logits.device)\n",
    "\n",
    "# IMPORTANT: assign back, and use -inf\n",
    "attn_logits = attn_logits.masked_fill(~mask, float(\"-inf\"))\n",
    "\n",
    "attn_probs = torch.softmax(attn_logits, dim=2)             # [B,T,S,N]\n",
    "context    = torch.einsum(\"BTSN,BSNH->BTNH\", attn_probs, value)\n",
    "output     = torch.einsum(\"BTNH,NHD->BTD\", context, output_w)\n",
    "\n",
    "print(output.shape)  # torch.Size([25, 50, 400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "195ab161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import math, torch, torch.distributed as dist\n",
    "\n",
    "# # ----- config -----\n",
    "# B, T = 25, 50\n",
    "# S = T\n",
    "# D = 8*50\n",
    "# N = 8   # query heads\n",
    "# K = 4   # kv heads\n",
    "# H = D // N\n",
    "# assert D % N == 0\n",
    "# P = dist.get_world_size()\n",
    "# rank = dist.get_rank()\n",
    "# assert N % P == 0 and K % P == 0, \"Heads must divide across TP ranks\"\n",
    "\n",
    "# N_p = N // P\n",
    "# K_p = K // P\n",
    "# G = N // K\n",
    "# G_local = N_p // K_p    # should equal G\n",
    "\n",
    "# # ----- data -----\n",
    "# device = torch.device(\"cuda\", index=rank) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# x = torch.rand((B, T, D), device=device)\n",
    "\n",
    "# # Shard weights by head axis\n",
    "# # global shapes: Q:(D,N,H), K/V:(D,K,H), O:(N,H,D)\n",
    "# Q_w_local = torch.rand((D, N_p, H), device=device)\n",
    "# K_w_local = torch.rand((D, K_p, H), device=device)\n",
    "# V_w_local = torch.rand((D, K_p, H), device=device)\n",
    "# O_w_local = torch.rand((N_p, H, D), device=device)  # row-parallel shard\n",
    "\n",
    "# # ----- forward on each rank -----\n",
    "# # Project to local heads\n",
    "# q = torch.einsum(\"BTD,DNH->BTNH\", x, Q_w_local)     # [B,T,N_p,H]\n",
    "# k = torch.einsum(\"BTD,DKH->BTKH\", x, K_w_local)     # [B,T,K_p,H]\n",
    "# v = torch.einsum(\"BTD,DKH->BTKH\", x, V_w_local)     # [B,T,K_p,H]\n",
    "\n",
    "# # GQA: expand local K/V to match local Q heads\n",
    "# k = k.repeat_interleave(G_local, dim=2)             # [B,T,N_p,H]\n",
    "# v = v.repeat_interleave(G_local, dim=2)             # [B,T,N_p,H]\n",
    "\n",
    "# # Attention per-head (local, no comms)\n",
    "# attn_logits = torch.einsum(\"BTNH,BSNH->BTSN\", q, k) # [B,T,S,N_p]\n",
    "# attn_logits = attn_logits / math.sqrt(H)\n",
    "\n",
    "# mask = torch.ones(T, S, dtype=torch.bool, device=device).tril().view(1,T,S,1)\n",
    "# attn_logits = attn_logits.masked_fill(~mask, float(\"-inf\"))\n",
    "\n",
    "# attn_probs = torch.softmax(attn_logits, dim=2)      # [B,T,S,N_p]\n",
    "# ctx_local  = torch.einsum(\"BTSN,BSNH->BTNH\", attn_probs, v)  # [B,T,N_p,H]\n",
    "\n",
    "# # Row-parallel output: each rank produces a partial [B,T,D], then sum-reduce\n",
    "# y_partial = torch.einsum(\"BTNH,NHD->BTD\", ctx_local, O_w_local)  # [B,T,D]\n",
    "# dist.all_reduce(y_partial, op=dist.ReduceOp.SUM)                 # -> [B,T,D]\n",
    "# y = y_partial  # final output on all ranks\n",
    "\n",
    "# # y shape is [B,T,D], matches your single-GPU result\n",
    "# print(rank, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88a3847",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "\n",
    "class MHA(nn.Module):\n",
    "    def __init__(self, d_model, num_q_heads, num_kv_heads):\n",
    "        super().__init__()\n",
    "        assert num_q_heads % num_kv_heads == 0\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_q_heads = num_q_heads\n",
    "        self.num_kv_heads = num_kv_heads    \n",
    "        self.head_dim = d_model // num_q_heads \n",
    "        self.group_size = num_q_heads // num_kv_heads \n",
    "\n",
    "        self.q_proj = nn.Parameter(torch.randn(d_model, num_q_heads, self.head_dim))\n",
    "        self.k_proj = nn.Parameter(torch.randn(d_model, num_kv_heads, self.head_dim))\n",
    "        self.v_proj = nn.Parameter(torch.randn(d_model, num_kv_heads, self.head_dim))\n",
    "        self.out_proj = nn.Parameter(torch.rand(num_q_heads, self.head_dim, d_model))\n",
    "    \n",
    "    def forward(self, x, causal_mask=True):\n",
    "        B, T, _ = x.shape \n",
    "        N, K, H = self.num_q_heads, self.num_kv_heads, self.head_dim \n",
    "        G = self.group_size \n",
    "\n",
    "        # compute Q, K, V\n",
    "        Q = torch.einsum(\"BTD,DNH->BTNH\", x, self.q_proj)  # [B,T,N,H]\n",
    "        K = torch.einsum(\"BTD,DKH->BTKH\", x, self.k_proj)  # [B,T,K,H]\n",
    "        V = torch.einsum(\"BTD,DKH->BTKH\", x, self.v_proj)  # [B,T,K,H]\n",
    "\n",
    "        # expand K/V for grouped query attention\n",
    "        K = K.repeat_interleave(G, dim=2)  # [B,T,N,H]\n",
    "        V = V.repeat_interleave(G, dim=2)  # [B,T,N,H]\n",
    "\n",
    "        # attention logits\n",
    "        attn_logits = torch.einsum(\"BTNH,BSNH->BTSN\", Q, K) / math.sqrt(H)  # [B,T,S,N]\n",
    "\n",
    "        # causal mask if needed\n",
    "        if causal_mask:\n",
    "            mask = torch.ones(T, T, dtype=torch.bool, device=x.device).tril().view(1, T, T, 1)\n",
    "            attn_logits = attn_logits.masked_fill(~mask, float(\"-inf\"))\n",
    "\n",
    "        attn_probs = torch.softmax(attn_logits, dim=2)  # [B,T,S,N]\n",
    "        context = torch.einsum(\"BTSN,BSNH->BTNH\", attn_probs, V)  # [B,T,N,H]\n",
    "\n",
    "        # output projection\n",
    "        out = torch.einsum(\"BTNH,NHD->BTD\", context, self.out_proj)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8620d355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GQAWithSlicedMask(nn.Module):\n",
    "    def __init__(self, d_model: int, num_q_heads: int, num_kv_heads: int, max_seq_len: int, causal: bool = True):\n",
    "        super().__init__()\n",
    "        assert num_q_heads % num_kv_heads == 0, \"num_q_heads must be divisible by num_kv_heads\"\n",
    "        assert d_model % num_q_heads == 0, \"d_model must be divisible by num_q_heads\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.N = num_q_heads\n",
    "        self.K = num_kv_heads\n",
    "        self.H = d_model // num_q_heads\n",
    "        self.G = self.N // self.K\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.causal = causal\n",
    "\n",
    "        # Projections\n",
    "        self.q_proj = nn.Parameter(torch.randn(d_model, self.N, self.H) / math.sqrt(d_model))\n",
    "        self.k_proj = nn.Parameter(torch.randn(d_model, self.K, self.H) / math.sqrt(d_model))\n",
    "        self.v_proj = nn.Parameter(torch.randn(d_model, self.K, self.H) / math.sqrt(d_model))\n",
    "        self.out_proj = nn.Parameter(torch.randn(self.N, self.H, d_model) / math.sqrt(self.N * self.H))\n",
    "\n",
    "        # Precompute full additive mask [1, Lmax, Lmax, 1]: 0 on/below diag, -inf above\n",
    "        if causal:\n",
    "            full = torch.full((max_seq_len, max_seq_len), float(\"-inf\"), dtype=torch.float32)\n",
    "            full = torch.tril(full, diagonal=0)  # keep lower triangle as -inf; fix below\n",
    "            # Set allowed (on/below diag) to 0\n",
    "            full[torch.tril(torch.ones_like(full, dtype=torch.bool))] = 0.0\n",
    "        else:\n",
    "            full = torch.zeros(max_seq_len, max_seq_len, dtype=torch.float32)\n",
    "        self.register_buffer(\"additive_mask_full\", full.view(1, max_seq_len, max_seq_len, 1), persistent=False)\n",
    "\n",
    "        # Scale buffer\n",
    "        self.register_buffer(\"scale\", torch.tensor(1.0 / math.sqrt(self.H), dtype=torch.float32), persistent=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, kv: torch.Tensor | None = None, padding_mask: torch.Tensor | None = None):\n",
    "        \"\"\"\n",
    "        x:  [B, T, D]           - queries\n",
    "        kv: [B, S, D] or None   - keys/values source (defaults to x for self-attn)\n",
    "        padding_mask: optional boolean mask, shape:\n",
    "            - self-attn: [B, T]  (True = keep, False = pad)\n",
    "            - cross-attn: [B, S]\n",
    "        Returns: [B, T, D]\n",
    "        \"\"\"\n",
    "        B, T, D = x.shape\n",
    "        assert D == self.d_model, f\"d_model mismatch: got {D}, expected {self.d_model}\"\n",
    "        if kv is None:\n",
    "            kv = x\n",
    "        S = kv.shape[1]\n",
    "        if T > self.max_seq_len or S > self.max_seq_len:\n",
    "            raise ValueError(f\"Sequence length exceeds max_seq_len ({self.max_seq_len}): T={T}, S={S}\")\n",
    "\n",
    "        # Projections\n",
    "        Q = torch.einsum(\"BTD,DNH->BTNH\", x, self.q_proj)   # [B,T,N,H]\n",
    "        K = torch.einsum(\"BSD,DKH->BSKH\", kv, self.k_proj)  # [B,S,K,H]\n",
    "        V = torch.einsum(\"BSD,DKH->BSKH\", kv, self.v_proj)  # [B,S,K,H]\n",
    "\n",
    "        # Expand K/V heads to N (GQA)\n",
    "        K = K.repeat_interleave(self.G, dim=2)  # [B,S,N,H]\n",
    "        V = V.repeat_interleave(self.G, dim=2)  # [B,S,N,H]\n",
    "\n",
    "        # Attention logits\n",
    "        attn_logits = torch.einsum(\"BTNH,BSNH->BTSN\", Q, K)  # [B,T,S,N]\n",
    "        attn_logits = attn_logits * self.scale.to(attn_logits.dtype)\n",
    "\n",
    "        # Add sliced causal mask (broadcasts to [B,T,S,N])\n",
    "        if self.causal:\n",
    "            mask_slice = self.additive_mask_full[:, :T, :S, :].to(attn_logits.dtype)\n",
    "            attn_logits = attn_logits + mask_slice\n",
    "\n",
    "        # Optional padding mask (True=keep, False=pad). Convert to additive.\n",
    "        if padding_mask is not None:\n",
    "            if padding_mask.shape[1] == S:   # key padding (common)\n",
    "                keep = padding_mask.view(B, 1, S, 1)  # -> [B,1,S,1], broadcast to [B,T,S,N]\n",
    "                add = torch.where(keep, 0.0, float(\"-inf\"))\n",
    "                attn_logits = attn_logits + add.to(attn_logits.dtype)\n",
    "            elif padding_mask.shape[1] == T: # query padding (rare, but supported)\n",
    "                keep = padding_mask.view(B, T, 1, 1)  # -> [B,T,1,1]\n",
    "                add = torch.where(keep, 0.0, float(\"-inf\"))\n",
    "                attn_logits = attn_logits + add.to(attn_logits.dtype)\n",
    "            else:\n",
    "                raise ValueError(\"padding_mask second dim must match T or S\")\n",
    "\n",
    "        attn_probs = torch.softmax(attn_logits, dim=2)        # [B,T,S,N]\n",
    "        context    = torch.einsum(\"BTSN,BSNH->BTNH\", attn_probs, V)  # [B,T,N,H]\n",
    "        out        = torch.einsum(\"BTNH,NHD->BTD\", context, self.out_proj)   # [B,T,D]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f4eb3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
