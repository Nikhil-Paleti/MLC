
# PyTorch Collective Communication â€“ Interview Notes

## ğŸ§© Core Collectives (Concepts)

| Collective | Purpose | Equivalent Composition | Communication Cost (bytes per process) |
|-------------|----------|------------------------|----------------------------------------|
| **All-Reduce** | Combine tensors (sum/avg/max) from all ranks â†’ everyone gets result | Reduce â†’ Broadcast | `O((p-1)/p * N)` â‰ˆ `2N` per process on ring |
| **Reduce** | Combine tensors and send result to one destination rank | N/A | `O((p-1)/p * N)` (only root receives result) |
| **Broadcast** | Send tensor from one source rank to all ranks | N/A | `O((p-1)/p * N)` |
| **All-Gather** | Each rank gathers tensors from all others (concatenation) | Gather â†’ Broadcast | `O((p-1) * N)` |
| **Reduce-Scatter** | Reduce (sum/avg) across ranks then scatter the result chunks | All-Reduce + Scatter | `O((p-1)/p * N)` |
| **All-to-All** | Each rank sends distinct chunk to every other rank | p simultaneous sends/recvs | `O((p-1)/p * N)` |
| **Barrier** | Synchronize all ranks (no data transfer) | Broadcast of zero bytes | Negligible |

---

## ğŸ” Intuition & Decomposition

| Operation | Conceptual Steps |
|------------|-----------------|
| **All-Reduce** | 1ï¸âƒ£ Each rank reduces (sums) partial results <br>2ï¸âƒ£ Result is broadcast back to all ranks |
| **Reduce-Scatter** | 1ï¸âƒ£ Perform an all-reduce <br>2ï¸âƒ£ Split the result into equal chunks (each rank keeps its own) |
| **All-Gather** | 1ï¸âƒ£ Each rank sends its chunk to everyone <br>2ï¸âƒ£ Equivalent to *Gather + Broadcast* |
| **All-to-All** | 1ï¸âƒ£ Each rank partitions tensor into p parts <br>2ï¸âƒ£ Exchanges them with all others (full matrix shuffle) |
| **Broadcast** | One-to-many replication (rank 0 â†’ all) |
| **Reduce** | Many-to-one aggregation (all â†’ rank 0) |

---

## ğŸš€ Communication Patterns (for GPUs via NCCL)

- **Ring algorithm** â€“ bandwidth-optimal for large tensors:  
  Time â‰ˆ `2 * (p-1)/p * (N / BW)`
- **Tree algorithm** â€“ low-latency for small tensors:  
  Time â‰ˆ `log(p)` Ã— per-message latency

---

## ğŸ§  Usage Cheat Snippets

```python
dist.all_reduce(tensor, op=dist.ReduceOp.SUM)
dist.reduce(tensor, dst=0)
dist.broadcast(tensor, src=0)

bufs = [torch.empty_like(tensor) for _ in range(world)]
dist.all_gather(bufs, tensor)

out = torch.empty_like(tensor)
dist.reduce_scatter(out, [tensor.clone() for _ in range(world)])

dist.all_to_all_single(out, tensor)
dist.barrier()
```

---

## ğŸ“Š Quick Comparison Summary

| Operation | Everyone gets full result? | Uses reduction? | Notes |
|------------|-----------------------------|------------------|-------|
| **Broadcast** | âœ… | âŒ | From one â†’ all |
| **Reduce** | âŒ | âœ… | From all â†’ one |
| **All-Reduce** | âœ… | âœ… | Combines + shares |
| **All-Gather** | âœ… | âŒ | Concatenates all |
| **Reduce-Scatter** | âŒ | âœ… | Memory-efficient all-reduce |
| **All-to-All** | âŒ | âŒ | Arbitrary shuffling |
| **Barrier** | âŒ | âŒ | Sync only |

---

## ğŸ’¡ Interview Nuggets

- `Reduce-Scatter + All-Gather = All-Reduce`
- DDP internally implements **All-Reduce** as **Reduce-Scatter + All-Gather**
- **All-to-All** â†’ used in MoE routing
- Use `async_op=True` for overlap
- Latency dominates small tensors; bandwidth dominates large tensors
