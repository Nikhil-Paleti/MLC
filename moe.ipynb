{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e2bcabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2f7f7236",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(embed_dim, hidden_dim, bias=False)\n",
    "        self.fc2 = nn.Linear(embed_dim, hidden_dim, bias=False)\n",
    "        self.fc3 = nn.Linear(hidden_dim, embed_dim, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # gated SwiGLU-style FFN: silu(W1x) * (W2x)\n",
    "        x_fc1 = self.fc1(x)\n",
    "        x_fc2 = self.fc2(x)\n",
    "        x = F.silu(x_fc1) * x_fc2\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "82646905",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 25\n",
    "T = 30\n",
    "D = 200\n",
    "N = 6 # no of experts \n",
    "E = 3 # no of experts per token \n",
    "\n",
    "inp = torch.rand((B,T,D))\n",
    "\n",
    "gate_proj = torch.rand((D, N))\n",
    "fc1 = [torch.rand((D, 400)) for _ in range(N)]\n",
    "fc2 = [torch.rand((D, 400)) for _ in range(N)]\n",
    "fc3 = [torch.rand((400, D)) for _ in range(N)]\n",
    "\n",
    "scores = (inp @ gate_proj).reshape(-1, N)\n",
    "topk_scores, topk_indices = torch.topk(scores, E, dim=-1)\n",
    "topk_probs = torch.softmax(topk_scores, dim=-1)\n",
    "\n",
    "inp_flat = inp.reshape(-1, D)\n",
    "output_flat = torch.zeros((B*T, D))\n",
    "\n",
    "unique_experts = torch.unique(topk_indices)\n",
    "for unique_expert_tensor in unique_experts:\n",
    "    unique_expert = unique_expert_tensor.item()\n",
    "    \n",
    "    mask = topk_indices == unique_expert\n",
    "    select_indices = mask.any(axis=-1)\n",
    "    \n",
    "    tokens_to_expert = inp_flat[select_indices, :]\n",
    "    output = F.silu(tokens_to_expert @ fc1[unique_expert]) * (tokens_to_expert @ fc2[unique_expert])\n",
    "    output = output @ fc3[unique_expert]\n",
    "\n",
    "    scale = topk_probs[select_indices,  mask[select_indices].to(torch.int).argmax(axis=-1)]\n",
    "    output *= scale[:, None]\n",
    "    \n",
    "    output_flat[select_indices, :] += output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1f805fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[30],\n",
      "        [40],\n",
      "        [80]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([\n",
    "    [10, 20, 30],\n",
    "    [40, 50, 60],\n",
    "    [70, 80, 90]\n",
    "])\n",
    "\n",
    "indices = torch.tensor([\n",
    "    [2],   # pick col 2,1,0 for row 0\n",
    "    [0],   # pick col 0,0,2 for row 1\n",
    "    [1]    # pick col 1,1,1 for row 2\n",
    "])\n",
    "\n",
    "y = torch.gather(x, dim=1, index=indices)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7c8007",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoEFeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.num_experts_per_tok = cfg[\"num_experts_per_tok\"]\n",
    "        self.num_experts = cfg[\"num_experts\"]\n",
    "        self.emb_dim = cfg[\"emb_dim\"]\n",
    "        self.gate = nn.Linear(cfg[\"emb_dim\"], cfg[\"num_experts\"], bias=False, dtype=cfg[\"dtype\"])\n",
    "\n",
    "        self.fc1 = nn.ModuleList([nn.Linear(cfg[\"emb_dim\"], cfg[\"moe_intermediate_size\"], bias=False, dtype=cfg[\"dtype\"])\n",
    "                                  for _ in range(cfg[\"num_experts\"])])\n",
    "        self.fc2 = nn.ModuleList([nn.Linear(cfg[\"emb_dim\"], cfg[\"moe_intermediate_size\"], bias=False, dtype=cfg[\"dtype\"])\n",
    "                                  for _ in range(cfg[\"num_experts\"])])\n",
    "        self.fc3 = nn.ModuleList([nn.Linear(cfg[\"moe_intermediate_size\"], cfg[\"emb_dim\"], bias=False, dtype=cfg[\"dtype\"])\n",
    "                                  for _ in range(cfg[\"num_experts\"])])\n",
    "\n",
    "    def forward(self, x):\n",
    "        scores = self.gate(x)  # (b, seq_len, num_experts)\n",
    "        topk_scores, topk_indices = torch.topk(scores, self.num_experts_per_tok, dim=-1)\n",
    "        topk_probs = torch.softmax(topk_scores, dim=-1)\n",
    "\n",
    "        batch, seq_len, _ = x.shape\n",
    "        x_flat = x.reshape(batch * seq_len, -1)\n",
    "        out_flat = torch.zeros(batch * seq_len, self.emb_dim, device=x.device, dtype=x.dtype)\n",
    "\n",
    "        topk_indices_flat = topk_indices.reshape(-1, self.num_experts_per_tok)\n",
    "        topk_probs_flat = topk_probs.reshape(-1, self.num_experts_per_tok)\n",
    "\n",
    "        unique_experts = torch.unique(topk_indices_flat)\n",
    "\n",
    "        for expert_id_tensor in unique_experts:\n",
    "            expert_id = int(expert_id_tensor.item())\n",
    "            mask = topk_indices_flat == expert_id\n",
    "            if not mask.any():\n",
    "                continue\n",
    "\n",
    "            token_mask = mask.any(dim=-1)\n",
    "            selected_idx = token_mask.nonzero(as_tuple=False).squeeze(-1)\n",
    "            if selected_idx.numel() == 0:\n",
    "                continue\n",
    "\n",
    "            expert_input = x_flat.index_select(0, selected_idx)\n",
    "            hidden = torch.nn.functional.silu(self.fc1[expert_id](expert_input)) * self.fc2[expert_id](expert_input)\n",
    "            expert_out = self.fc3[expert_id](hidden)\n",
    "\n",
    "            mask_selected = mask[selected_idx]\n",
    "            slot_indices = mask_selected.int().argmax(dim=-1, keepdim=True)\n",
    "            selected_probs = torch.gather(topk_probs_flat.index_select(0, selected_idx), dim=-1, index=slot_indices).squeeze(-1)\n",
    "\n",
    "            out_flat.index_add_(0, selected_idx, expert_out * selected_probs.unsqueeze(-1))\n",
    "\n",
    "        return out_flat.reshape(batch, seq_len, self.emb_dim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
